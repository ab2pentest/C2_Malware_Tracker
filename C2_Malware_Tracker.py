#/usr/bin/python3

# C2 Malware Tracker By AB2
# Big thanks to: https://github.com/kadnass-dz

import requests
import os
from bs4 import BeautifulSoup
from beautifultable import BeautifulTable
import json

# The Main Menu :D

print(" C2 Malware Tracker ".center(70,"-"))
print("Available Sources: ")
print("	1 - cybercrime-tracker.net ")
print("	2 - threatshare.io ")

##########################################
choice = input("Choose Source [1 or 2]: ")
keyword = input("Enter Malware/Keyword [example: Lokibot] : ")
saveas = input("Save output as [example: results.txt] : ")
##########################################

if choice == "1" :
	
	req = requests.get("http://cybercrime-tracker.net/index.php?search="+keyword)
	src = req.content
	s = BeautifulSoup(src,"lxml")
	pg_number = s.find_all("div",{"id":"selector"} )
	# get pagination :D
	print("-"*70)
	s = str(pg_number[0]).split("<a href=\"index.php?s=")
	final_namber=s[4].split("&")
	f = int(final_namber[0])
	a = 0
	z = 0
	x=f/40 
	links = []
	date =[]
	ip =[]
	family =[]

	while a < f-40:
		z += 1
		xx=z/x*100
		# get content
		r = requests.get(f"http://cybercrime-tracker.net/index.php?s={a}&m=40&search={keyword}")
		sourc = r.content
		s = BeautifulSoup(sourc,"lxml")
		link = s.find_all("td" )
		# get url
		b = 6
		while b < len(link):		
			links.append(link[b].text)
			b+=5
		
		# get date
		b = 5
		while b < len(link):		
			date.append(link[b].text)
			b+=5
		# get IP	
		b = 7
		while b < len(link):		
			ip.append(link[b].text)
			b+=5
		# get Family
		b = 8
		while b < len(link):		
			family.append(link[b].text)
			b+=5
		# %
		print(f"{r} in Processing % {int(xx)}")
		a += 40
	#table
	table = BeautifulTable()
	table.columns.header = ["Date", "Url", "IP" ,"Family"]
	for i in range(len(links)):
		table.rows.append([date[i],links[i] , ip[i] ,family[i]])
	# Printing Total of results
	print(f"Total Result :{len(links)}")
	# Printing results
	print(table)
	# Saving results
	file= open(saveas,'w')
	file.write(str(table))
	file.close()
	print(f"Result saved as: {saveas}")
	
elif choice == "2" :
	req = requests.get(f"https://threatshare.io/malwaredata?draw=6&columns%5B0%5D%5Bdata%5D=family&columns%5B0%5D%5Bname%5D=&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=url&columns%5B1%5D%5Bname%5D=&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=ip&columns%5B2%5D%5Bname%5D=&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=status&columns%5B3%5D%5Bname%5D=true&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=false&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=malbeacon&columns%5B4%5D%5Bname%5D=&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=false&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=date&columns%5B5%5D%5Bname%5D=&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&start=0&length=99999&search%5Bvalue%5D={keyword}&search%5Bregex%5D=false&_=1644346457659")
	src = req.content
	s = BeautifulSoup(src,"html.parser").get_text()
	d=json.loads(s)
	table = BeautifulTable()
	table.columns.header = ["Date", "Url", "IP" ,"Family" ,'Status']
	for key in d['data'] :
			table.rows.append([key['date'],key['url'] , key['ip'] ,key['family'],key['status']])
	# Printing results
	print(table)
	# Saving results
	file= open(saveas,'w')
	file.write(str(table))
	file.close()
	print(f"Result saved as: {saveas}")
else :
	print("Please enter a valid choice !")
	exit()

print("*"*70)
print("Don't forget to give us a star at: https://github.com/ab2pentest/C2_Malware_Tracker")
